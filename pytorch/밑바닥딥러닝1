# -*- coding: utf-8 -*-
"""deeplearning_bottom.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fZL1kpyv8WlUJ9kPDRb3u-Ev4K-1FUga
"""

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

import numpy as np
import matplotlib.pylab as plt
import sys, os
sys.path.append(r'C:\Users\vhbncm\Desktop\3학년1학기 공부\밑바닥딥러닝\deep-learning-from-scratch-master\deep-learning-from-scratch-master') # 부모 디렉토리의 파일을 가져올 수 있도록 설정
from dataset.mnist import load_mnist
import numpy as np
from PIL import Image
from common.functions import softmax, cross_entropy_error # common/functions.py에 정의한 softmax, cross_entropy_error 메서드를 이용 
from common.gradient import numerical_gradient
from common.functions import *
from common.gradient import numerical_gradient
from common.util import im2col #7장 CNN


#퍼셉트론 구현하기
def AND(x1, x2):
  w1, w2, theta = 0.5, 0.5, 0.7
  tmp = x1*w1 + x2*w2
  if tmp <= theta:
    return 0
  elif tmp > theta:
    return 1
AND(0, 0)

import numpy as np
x = np.array([0, 1]) #입력
w = np.array([0.5, 0.5]) #가중치
b = -0.7 # 편향
w*x
np.sum(w*x)
np.sum(w*x) + b

def AND(x1, x2):
  x = np.array([x1, x2])
  w = np.array([0.5, 0.5])
  b = -0.7
  tmp = np.sum(w*x) + b
  if tmp <= 0:
    return 0
  elif tmp > 0:
    return 1

# AND 게이트에서 가중치 및 편향의 부호를 반대로 바꾸면 NAND게이트가 된다.
def NAND(x1, x2):
  x = np.array([x1, x2])
  w = np.array([-0.5, -0.5])
  b = 0.7
  tmp = np.sum(w*x) + b
  if tmp <= 0:
    return 0
  else: 
    return 1

def OR(x1, x2):
  x = np.array([x1, x2])
  w = np.array([0.5, 0.5]) #AND와는 가중치(W, b)만 다르다!
  b = -0.2
  tmp = np.sum(w*x) + b
  if tmp <= 0:
    return 0
  else:
    return 1
    
#but 이러한 퍼셉트론은 선형 영역만 표시할 수 있다.
#XOR같은 비선형은 표현할 수 없다. 
#but 다층 퍼셉트론으로 NAND OR을 입력으로 AND의 출력을 받으면 XOR이 된다.

def XOR(x1, x2):
  s1 = NAND(x1, x2)
  x2 = OR(x1, x2)
  y = AND(s1, s2)
  return y

#%%3장
#계단함수 구현하기
def step_function(x):
  if x> 0:
    return 1
  else:
    return 0

#배열까지 처리 가능한 함수
def setp_function(x):
  y = x > 0
  return y.astype(np.int) #y를 bool형에서 int형으로 바꿔줌

x = np.array([-1.0, 1.0 ,2.0])
y = x > 0
y

def step_function(x):
  return np.array(x > 0, dtype = np.int)
x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()

#시그모이드 구현
def sigmoid(x):
  return 1/(1 + np.exp(-x))

x = np.array([-1.0, 1.0, 2.0])
sigmoid(x)

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()

#ReLU
def relu(x):
  return np.maximum(0, x)

#각 층의 신호전달 구현하기
X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

print(W1.shape)
A1 = np.dot(X, W1) + B1
Z1 = sigmoid(A1)
print(A1)
print(Z1)

#1층에서 2층으로 가는 과정 
W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
B2 = np.array([0.1, 0.2])
A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)

def identity_function(x):
  return x
  
W3 = np.array([[0.1, 0.3], [0.2, 0.4]])
B3 = np.array([0.1, 0.2])
A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)

# 구현 정리 
def identity_function(x):
  return x
def init_network():
  network = {}
  network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
  network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
  network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
  network['b1'] = np.array([0.1, 0.2, 0.3])
  network['b2'] = np.array([0.1, 0.2])
  network['b3'] = np.array([0.1, 0.2])
  return network

def forward(network, x):
  W1, W2, W3 = network['W1'], network['W2'], network['W3']
  b1, b2, b3 = network['b1'], network['b2'], network['b3']

  a1 = np.dot(x, W1) + b1
  z1 = sigmoid(a1)
  a2 = np.dot(z1, W2) + b2
  z2 = sigmoid(a2)
  a3 = np.dot(z2, W3) + b3
  y = identity_function(a3)

  return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y)

#softmax 함수 구현
a = np.array([0.3, 2.9, 4.0])
exp_a = np.exp(a)
print(exp_a)

sum_exp_a = np.sum(exp_a)
y = exp_a / sum_exp_a
print(y)

#exp값에서 overflow가 나지 않도록 softmax의 지수함수를 계산시 정수를 더하고 빼도 상관 없다.
a = np.array([1010, 1000, 990])
np.exp(a) / np.sum(np.exp(a))
c = np.max(a)
#최대값을 빼줌 
a - c
np.exp(a - c) / np.sum(np.exp(a - c))



#경로변경
#%%
#MNIST dataset
import sys, os
sys.path.append(r'C:\Users\vhbncm\Desktop\3학년1학기 공부\밑바닥딥러닝\deep-learning-from-scratch-master\deep-learning-from-scratch-master') # 부모 디렉토리의 파일을 가져올 수 있도록 설정
from dataset.mnist import load_mnist
import numpy as np
from PIL import Image

(x_train, t_train) , (x_test, t_test) = \
  load_mnist(flatten = True, normalize = False)

print(x_train.shape) 
print(t_train.shape) 
print(x_test.shape) 
print(t_test.shape)

#%%
def img_show(img):
    pil_img = Image.fromarray(np.uint8(img)) #Image.fromarray 로 넘파이로 저장된 이미지 데이터를 PIL용 데이터 객체로 변환한다.
    pil_img.show()
    
(x_train, t_train), (x_test, t_test) = \
    load_mnist(flatten = True, normalize = False)
    
img = x_train[0]
label = t_train[0]
print(label) 

print(img.shape)
img = img.reshape(28, 28) #flatten 되어있기 때문에  원래 이미지 모형으로 변경
print(img.shape)

img_show(img)

#%%
def get_data():
    (x_train, t_train), (x_test, t_test) = \
        load_mnist(flatten = True, normalize = False)
    return x_test, t_test

def init_network():
    with open("sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
        
    return network

def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)
    
    return y

#%%
x, t = get_data()
network = init_network()

accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p = np.argmax(y) #확률이 가장 높은 원소의 인덱스를 얻는다.
    if p == t[i]:
        accuracy_cnt += 1
        
print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

#%%
x, _ = get_data()
network = init_network()
W1, W2, W3 = network['W1'], network['W2'], network['W3']

x.shape
x[0].shape
W1.shape
W2.shape
W3.shape

#%% 배치처리 구현 -> 더 효율적이다. 
x, t = get_data()
network = init_network()

batch_size = 100 #배치 크기
accuracy_cnt = 0

for i in range(0, len(x), batch_size):
    x_batch = x[i:i+batch_size]
    y_batch = predict(network, x_batch)
    p = np.argmax(y_batch, axis = 1)
    accuracy_cnt += np.sum(p == t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

#%%4단원
#손실함수의 종류로 오차제곱합 교차 엔트로피 오차를 사용함
#오차제곱합

def sum_squares_error(y, t):
    return 0.5*np.sum((y-t)**2)
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
sum_squares_error(np.array(y), np.array(t))

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
sum_squares_error(np.array(y), np.array(t))

#첫번째 경우가 오차가 더 작아 정답에 더 가까울 것으로 알 수 있다.

#%%교차 엔트로피 오차
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))

#%%mini -batch
#손실함수를 batch에서만 값을 구해 효율성을 높임.
import sys, os
sys.path.append(r'C:\Users\vhbncm\Desktop\3학년1학기 공부\밑바닥딥러닝\deep-learning-from-scratch-master\deep-learning-from-scratch-master') # 부모 디렉토리의 파일을 가져올 수 있도록 설정
(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize = True, one_hot_label = True)
print(x_train.shape)
print(t_train.shape)
    

train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]

#%%(batch용)교차 엔트로피 오차 구현하기
#정답 레이블이 원-핫 인코딩일 경우
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size

#정답 레이블이 숫자 레이블로 주어졌을 경우
## 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로 그 계산은 무시해도 좋다. 
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
#y[np.arange(batch_size), t] : 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출한다. 

#%%수치 미분
def numerical_diff(f, x):
    h = 1e-4
    return (f(x+h) - f(x - h) ) / (2*h)

def function_1(x):
    return 0.01*x**2 + 0.1*x

x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
plt.xlabel("x")
plt.ylabel("f(x)")
plt.plot(x, y)
plt.show()

numerical_diff(function_1, 5)

#%%편미분 (x0 = 3, x1= 4에서 x0에 대한 편미분을 구하라 )
def function_tmp1(x0):
    return x0*x0 + 4.0**2.0

numerical_diff(function_tmp1, 3.0)

#%%기울기
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성
    
    for idx in range(x.size):
        tmp_val= x[idx]
        #f(x+h) 계산
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        #f(x-h) 계산
        x[idx] = tmp_val - h
        fxh2 = f(x)
        
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val #값 복원
    return grad
        
#%%경사하강법
#lr : 학습률 step_num = 반복 횟수
def gradient_descent(f, init_x, lr = 0.01, step_num = 100):
    x = init_x 
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x

#%% 신경망을 통해 기울기 구하기
#클래스 simpleNet의 메서드는 3개 
class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2,3) # 정규분포로 초기화
 
    def predict(self, x):
        return np.dot(x, self.W)
    
    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)
        
        return loss

net = simpleNet()
print(net.W) #가중치 매개변수
x = np.array([0.6, 0.9])
p = net.predict(x)
print(p)
np.argmax(p) #최댓값의 인덱스

t= np.array([0, 0, 1]) #정답 레이블
net.loss(x, t)

#기울기
def f(W):
    return net.loss(x, t)

#간단한 함수의 경우 lambda func 사용 
f = lambda w: net.loss(x, t)
dW = numerical_gradient(f, net.W) #여기서 numerical_gradient는 가중치 매개변수 W가 다차원 배열을 처리할 수 있도록 앞의 구현에서 조금 수정함. 

dW = numerical_gradient(f, net.W)
print(W)

#%%2층 신경망 클래스 구현하기

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size,
                 weight_init_std = 0.01):
        #가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * \
                            np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * \
                            np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
    
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        
        b1, b2 = self.params['b1'], self.params['b2']
    
        a1 = np.dot(x,W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1,W2) + b2
        y = softmax(a2)
    
        return y
#x: 입력 데이터, t: 정답 레이블

    def loss(self, x, t):
        y= self.predict(x)
        
        return cross_entropy_error(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis = 1)
        t = np.argmax(t, axis = 1)
        
        accuracy = np.sum(y == t) / float(x, shape[0])
        return accuracy
    #x: 입력데이터 t: 정답 레이블
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
    

# 가중치 매개변수와 기울기 값을 확인 
net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)
net.params['W1'].shape
net.params['b1'].shape
net.params['W2'].shape
net.params['b2'].shape

x = np.random.randn(100, 784)
y = net.predict(x)

x = np.random.randn(100, 784)
t = np.random.randn(100, 10)

grads = net.numerical_gradient(x, t)
grads['W1'].shape
grads['b1'].shape
grads['W2'].shape
grads['b2'].shape

#%%미니배치 학습 구현하기
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize = True, one_hot_label = True)
    
train_loss_list = []
train_acc_list = []
test_acc_list = []

#1에폭당 반복 수
iter_per_epoch = max(train_size / batch_size, 1)


#하이퍼파라미터
iters_num = 10000 #반복횟수
train_size = x_train.shape[0]
batch_size = 100 #미니배치 크기
learning_rate = 0.1

network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)

for i in range(iters_num):
    #미니배치 휙득
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    #기울기 계산
    grad = network.numerical_gradient(x_batch, t_batch)
    # grad = network.gradient(x_batch, t_batch) # 성능 개선판! 
    
    #매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
        
    #학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    #1에폭당 정확도 계산 -> overfitting이 일어나는가 확인 
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + "," + str(test_acc))
#train accuracy 와 test accuracy가 비슷하게 유지됨 -> overfitting이 없다 
#%%7장 CNN
import numpy as np
#4차원 배열
x = np.random.rand(10, 1, 28, 28) #무작위로 데이터 생성
x.shape
x[0] #1번째 데이터
x[0,0]# 또는 x[0][0] 1번째 데이터의 첫 채널의 공간 데이터에 접근

#for문 대신 im2col 사용
#im2col은 4차원 데이터를 2차원으로 바꿔준다.
#im2col(input_data, filter_h, filter_w, stride = 1, pad = 0)
#input_data :입력 데이터
#filter_h : 필터의 높이
#filter_w : 필터의 너비


x1 = np.random.rand(1, 3, 7, 7) #(데이터의 수 , 채널 수, 높이, 너비)
col1 = im2col(x1, 5, 5, stride = 1, pad = 0)
print(col1.shape) 

x2 = np.random.rand(10, 3, 7, 7) #데이터 10개
col2 = im2col(x2, 5, 5, stride = 1, pad = 0)
print(col2.shape)

#%%합성곱 계층의 forward 구현
class Convolution:
    def __init__(self, W, b, stride = 1, pad = 0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
        
    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)
        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)
        
        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T #필터 전개
        out = np.dot(col, col_W) + self.b
        
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)
        
        return out

#%%합성곱 계층의 역전파는 common/layer.py에서 공부하기

#%%풀링 구현
class Pooling:
    def __init__(self, pool_h, pool_w, stride = 1, pad = 0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
    
    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)

        #전개 (1)
        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h*self.pool_w)
        
        #최댓값 (2)
        out = np.max(col, axis = 1)
        
        #성형 (3)
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
        
        return out
    
#%%풀링 역전파는 common/layer.py를 참고


#%%CNN구현하기





























